# âš½ Football Club Data Crawler (Python)
## ğŸ“– Overview
This project is a **data crawling system** that collects **football club information** from **public web sources** by city.  
It started as a **basic Selenium-based crawler (v1)**, was enhanced with **multi-browser parallelism** for better efficiency (**v2**),  
and ultimately evolved into an **asynchronous API-powered crawler (v3)** â€” achieving **high performance âš¡**, **scalability ğŸŒ**, and **production-level stability ğŸ§©**.

## âš™ï¸ Technologies
- **Python 3.9+**
- **Selenium (v1)** â€” Browser automation for crawling web content
- **Selenium + Threading (v2)** â€” Multi-browser concurrent crawling for higher throughput
- **Asyncio + httpx (v3)** â€” Fully asynchronous API requestâ€“based crawler
- **Multiprocessing + AsyncIO hybrid execution**
- **Adaptive concurrency limiter** to balance performance and server stability
- **Pickle-based caching & checkpoint recovery** for safe resume after interruptions
- **Retry logic & exponential backoff** for network stability
- **Logging system + tqdm progress bar** for real-time monitoring
---

## ğŸ§© Project Structure
`````
FootballClubDataCrawler/
â”‚
â”œâ”€â”€ city_crawling.py # Step 1: Crawl city list
â”‚
â”œâ”€â”€ club_crawling_v1.py # Step 2 (Version 1): Basic Selenium crawler
â”œâ”€â”€ club_crawling_v2.py # Step 2 (Version 2): Optimized Selenium + multi-browser
â”œâ”€â”€ club_crawling_v3.py # Step 2 (Version 3): Async + API-based high-performance crawler
â”‚
â”œâ”€â”€ requirements_v1_v2.txt # Dependencies for v1
â”œâ”€â”€ requirements_v1_v2.txt # Dependencies for v2 (Selenium optimized)
â”œâ”€â”€ requirements_v3.txt # Dependencies for v3 (Async API version)
â”‚
â”œâ”€â”€ .env # Environment variables (API base URL, output path, etc.)
â”œâ”€â”€ logs/ # Folder for runtime logs
â”œâ”€â”€ data/ # Folder for city lists, output CSV files
â”œâ”€â”€ storage/ # Folder for cache files (pickle, checkpoints)
â””â”€â”€ README.md # Project documentation
`````
## ğŸ§  Conceptual Flow
1. **`city_crawling.py`** â€” Collects all available cities that contain football clubs and saves them as a CSV or list file.  
2. **`club_crawling_v1.py`** (Selenium) â€” Opens the browser, loads each city page, and extracts club data (names, age groups, types, etc.), but **highly optimized** for better performance and reliability compared to early versions..
3. **`club_crawling_v2.py`** (Selenium) â€” Opens the browser, loads each city page, and extracts club data (names, age groups, types, etc.).
4. **`club_crawling_v3.py`** (Async + API-based) â€” Uses API endpoints directly with asynchronous HTTP requests to crawl data **in parallel**, while maintaining server stability using **adaptive rate control** and retry logic.

## ğŸš€ Setup
### ğŸ§° 1. Install Python
Install Python 3.9 or above: [https://www.python.org/downloads/](https://www.python.org/downloads/)
### âš™ï¸ 2. Install Dependencies
#### ğŸ§© For Version 1 (Selenium)
```
pip install -r requirements_v1_v2.txt
```
#### ğŸ§© For Version 2 (Selenium)
```
pip install -r requirements_v1_v2.txt
```
#### âš¡ For Version 3 (Async + API-based)
- **On Windows:**
```
pip install -r requirements_v3.txt
```
- **On Linux/macOS (Recommended for max performance):**
```
pip install -r requirements_v3.txt
```
```
pip install uvloop
```
> âš ï¸ Note: `uvloop` cannot be installed on Windows but can **boost async performance by 30â€“50%** on Linux/macOS.

### ğŸ§¾ Example `.env` File
```
API_CLUB_RECOMMENDATION_URL = "https://example.com/api"
API_CLUB_INFO_URL = "https://example.com/api"
KEY_CLUB_INFO_AND_RECOMMENDATION_INFO =  ""
KEY_CLUB_CONTACT_INFO = ""
WEBSITE_URL = "https://find.englandfootball.com/"
WEBSITE_NAME = "EnglandFootballClub"
MAX_PROCESSES = 5
MAX_CONCURRENT_REQUESTS = 10000000000
BATCH_SAVE_SIZE = 200
RATE_LIMIT_SLEEP = 60
```

### ğŸ§© Step 2A â€” Crawl Clubs Using Selenium (v1)
```
python city_crawling.py
```
### ğŸ§© Step 2A â€” Crawl Clubs Using Selenium (v1)
```
Output: `data/clubs_data.csv`
```
Output: `data/clubs_data.csv`

## ğŸ§© Version Comparison

| Feature / Aspect | v1 â€” Basic Selenium | v2 â€” Optimized Selenium (Multi-Browser) | v3 â€” Async API Request |
|------------------|---------------------|------------------------------------------|-------------------------|
| **Core Technology** | Selenium WebDriver | Selenium (parallel multi-browser) | Asyncio + HTTPX (direct API requests) |
| **Speed** | ğŸ¢ Slow â€” single browser, sequential | âš™ï¸ Moderate â€” faster with parallel threads | âš¡ Extremely fast â€” async + connection pooling |
| **Performance Control** | None | Limited concurrency tuning | Adaptive limiter + retry with exponential backoff |
| **System Resource Usage** | Very high (1 browser per thread) | High (multi-browser consumes more CPU/RAM) | Low (non-blocking I/O, lightweight) |
| **Stability** | Prone to crashes or freezes | Improved but still limited by browser instability | Highly stable with caching & auto retries |
| **Error Recovery** | Manual rerun required | Partial recovery via logging | Full auto-resume using cache & pickle checkpoints |
| **Maintainability** | Simple but inefficient | More complex with multi-thread setup | Clean structure, easier to scale or integrate |
| **Scalability** | Poor â€” limited by Selenium overhead | Moderate â€” improved via concurrency | Excellent â€” supports distributed crawling |
| **Use Case** | Prototype / testing phase | Medium-scale crawling | Production-grade large-scale crawling |
| **Compatibility** | Works on all OS | Works on all OS | Best on Linux/macOS (with `uvloop` optimization) |


## ğŸ§© Requirements Summary
### `requirements_v3.txt`

*(Optional for Linux/macOS only)*
```
pip install uvloop
```

## ğŸ¯ Learning & Technical Takeaways
| Concept | Description |
|----------|-------------|
| **Asynchronous Programming (asyncio)** | Handles thousands of requests concurrently without blocking. |
| **Multiprocessing** | Runs multiple cities in parallel using CPU cores. |
| **Adaptive Rate Limiting** | Adjusts speed automatically when the target server slows down. |
| **Pickle Caching** | Saves progress and skips duplicates. |
| **Retry & Error Handling** | Retries failed requests with exponential backoff. |
| **Logging & tqdm** | Real-time progress tracking. |

## ğŸ Author
**Linh Nguyen Hoang**  
ğŸ¯ Aspiring Python Backend / Data Engineer
ğŸŒ Goal: Build a high-performance asynchronous crawler to collect and process structured football club data efficiently.
ğŸ“§ Contact: *[alinh1803@gmail.com]*  
## ğŸ“œ License
MIT License Â© 2025 Linh Nguyen Hoang